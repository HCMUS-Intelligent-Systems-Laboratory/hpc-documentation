[[{"l":"Hướng dẫn sử dụng SLURM cho người dùng cuối","p":["SLURM (Simple Linux Utility for Resource Management) là một hệ thống quản lý tài nguyên và điều phối tác vụ (job scheduler) mã nguồn mở, được sử dụng rộng rãi trong các hệ thống tính toán hiệu năng cao (High Performance Computing - HPC). Thay vì để từng người dùng truy cập trực tiếp vào tài nguyên tính toán (như các node hay CPU), SLURM giúp phân phối tài nguyên một cách công bằng, hiệu quả, và dễ quản lý.","Người dùng nên làm quen với cú pháp và các lệnh cơ bản để sử dụng SLURM hiệu quả. Hãy thường xuyên tham khảo tài liệu và các hướng dẫn cụ thể khi cần thiết để giải quyết các vấn đề phát sinh trong quá trình sử dụng."]}],[{"l":"Khái niệm và yêu cầu cơ bản"},{"l":"Các khái niệm cơ bản","p":["Job: Tác vụ hoặc chương trình bạn muốn hệ thống chạy.","Node: Một máy tính riêng biệt trong cụm máy (cluster)."]},{"l":"Yêu cầu cơ bản để sử dụng SLURM","p":["Trước khi bắt đầu, người dùng cần nắm vững một số kiến thức cơ bản về Linux và cái nhìn tổng quan về hệ thống HPC.","Hệ thống server HPC hiện tại bao gồm:","02 login nodes: chỉ dùng để đăng nhập thông qua ssh, lưu trữ mã nguồn và dữ liệu, nộp job lên các compute nodes, không dùng cho việc chạy task/job xử lý dữ liệu hoặc huấn luyện mô hình.","Node login01 có IP: 172.29.74.80","Node login02 có IP: 172.29.74.81","05 compute nodes: có nguồn tài nguyên lớn bao gồm CPUs, RAM và GPUs, được điều phối tự động thông qua SLURM để chạy các task/job, người dùng không thể đăng nhập hay kết nối trực tiếp đến các compute nodes.","gpu01, gpu02, gpu03, gpu04, gpu05"]}],[{"l":"Chạy một job đơn giản","p":["Dùng srun chạy trực tiếp:","srun là lệnh được dùng để chạy một chương trình hoặc lệnh trực tiếp trên hệ thống tính toán sử dụng SLURM. Khi dùng srun, bạn sẽ yêu cầu tài nguyên (CPU, RAM, GPU,...) và SLURM sẽ cấp phát tài nguyên tương ứng để chạy chương trình. srun phù hợp cho các tác vụ nhanh, thử nghiệm hoặc chạy tương tác trong môi trường dòng lệnh. Ví dụ được minh họa bên dưới.","Yêu cầu 4GB RAM và 1 GPU:","Khi chạy lệnh trên, yêu cầu sẽ vào hàng đợi, nếu SLURM tìm đủ tài nguyên để cấp phát, job này sẽ được khởi chạy và khi đó người dùng sẽ được kết nối đến một trong các compute nodes và truy cập tới vùng tài nguyên được yêu cầu bao gồm 4GB RAM và 1 GPU.","Dùng sbatch để gửi script:","sbatch là lệnh dùng để gửi một script (thường là file .sh) vào hệ thống SLURM để chạy không đồng bộ. Nghĩa là bạn không cần chờ kết quả ngay trên terminal - hệ thống sẽ xếp hàng, cấp phát tài nguyên khi có thể và chạy script của bạn. sbatch phù hợp cho các tác vụ dài, tốn nhiều thời gian, phức tạp hoặc cần nhiều tài nguyên.","Một script SLURM cần phải bao gồm ba phần chính:","Quy định tài nguyên: Kích thước bộ nhớ, số lượng CPU cores, số nút, thời gian chạy tối đa.","Thiết lập môi trường: Tải các module cần thiết hoặc kích hoạt môi trường ảo.","Lệnh thực thi: Lệnh mà người dùng muốn chạy.","Dưới đây là một ví dụ về script SLURM để chạy một chương trình Python:","Để gửi một job, người dùng sử dụng lệnh sbatch:","Trong đó job.slurm là tên của file script SLURM. Để kiểm tra kết quả của quá trình chạy script bằng lệnh sbatch, các output của các câu lệnh trong script sẽ được ghi vào 1 file job_id.out.","Lưu ý quan trọng: Để tiến hành huấn luyện một mô hình AI hoặc chạy một script nào đó trên SLURM, bạn nên chạy các câu lệnh của mình bằng lệnh srun trước và đảm bảo rằng toàn bộ các câu lệnh trong script của bạn chạy đúng và trả ra kết quả, sau đó bạn đưa các câu lệnh theo đúng trình tự vào script của lệnh sbatch và nộp lên để SLURM xử lý ngầm."]},{"l":"Giới thiệu Slurm","p":["Trên tất cả các hệ thống, người dùng chạy chương trình bằng cách gửi script tới Slurm. Slurm script phải thực hiện ba việc:","Quy định các yêu cầu về tài nguyên cho job","cài đặt môi trường","Chỉ định job sẽ được thực hiện dưới dạng lệnh shell","Dưới đây là tập lệnh Slurm mẫu để chạy mã Python bằng môi trường Conda:","Dòng đầu tiên của tập lệnh Slurm chỉ định shell Unix sẽ được sử dụng.","Tiếp theo là một loạt lệnh #SBATCH đặt ra các yêu cầu về tài nguyên và các tham số khác của job.","Tập lệnh trên yêu cầu 1 lõi CPU và 4 GB bộ nhớ trong thời gian chạy 1 phút. Những thay đổi cần thiết đối với môi trường được thực hiện bằng cách tải module môi trường anaconda3/ và kích hoạt một môi trường Conda cụ thể.","Cuối cùng, job cần thực hiện, tức là thực thi tập lệnh Python, được chỉ định ở dòng cuối cùng.","Xem bên dưới để biết thông tin về sự tương ứng giữa các tác vụ và lõi CPU. Nếu job của bạn không hoàn thành trước thời hạn quy định thì nó sẽ bị hủy. Bạn nên sử dụng giá trị chính xác cho thời gian ước lượng nhưng thêm thêm 20% để đảm bảo an toàn.","Một tập lệnh job có tên job.slurm được gửi tới bộ lập lịch Slurm bằng lệnh sbatch:","Job phải được gửi đến từ head node của một cluster. Job scheduler sẽ sắp xếp job ở nơi nó sẽ giữ nguyên cho đến khi nó có đủ mức độ ưu tiên để chạy trên compute node. Tùy thuộc vào tính chất job và nguồn lực sẵn có, thời gian xếp hàng sẽ thay đổi từ vài giây đến nhiều ngày. Để kiểm tra trạng thái của các job được xếp hàng đợi và đang chạy, hãy sử dụng lệnh sau:","Để xem thời gian bắt đầu dự kiến của các job của bạn:"]},{"i":"các-câu-lệnh-slurmthường-dùng","l":"Các câu lệnh Slurm thường dùng"},{"l":"Tìm hiểu trước về hệ thống","p":["\"quality of service\" - xem cách phân chia job và giới hạn trên mỗi phân vùng","cat /etc/os-release","checkquota (chưa hoạt động)","hiển thị các chia sẻ cụm được chỉ định bởi nhóm","hiển thị hoạt động của bộ xử lý, các lệnh đang chạy (nhấn 'q' để thoát)","hiển thị hoạt động của bộ xử lý, các lệnh đang chạy, có màu sắc (nhấn 'q' để thoát)","htop","Lệnh","lscpu","Mô tả","qos","shownodes","sinfo","sininfo -p gpu","snodes","sprio -w","sshare","thông tin về các nút tính toán","thông tin về các nút tính toán (dễ đọc hơn)","thông tin về CPU trên nút cụ thể đó","thông tin về hệ điều hành","top","xem GPU đang được sử dụng như thế nào","xem mức độ ưu tiên job được chỉ định như thế nào (hiển thị trọng số cho từng yếu tố)","xem tất cả các nút của cụm đang được sử dụng như thế nào (ví dụ: chúng có ở trạng thái rảnh không? ngừng hoạt động? đang được sử dụng?)","xem việc sử dụng dung lượng ổ đĩa của bạn, kiểm tra xem bạn có đủ dung lượng trong các thư mục của mình không, liên kết ở phía dưới để yêu cầu thêm dung lượng"]},{"l":"Gửi một Job","p":["Lệnh","Mô tả","sbatch","gửi job của bạn tới job scheduler","srun","yêu cầu một interactive job trên computing node (xem bên dưới)"]},{"l":"Sau khi Job được submit","p":["báo cáo thời gian bắt đầu dự kiến ​​cho các job đang chờ xử lý","hiển thị các nút đang được sử dụng cho job đang chạy của bạn","hiển thị job hiện tại","hiển thị mức độ ưu tiên được giao cho các job đang chờ xử lý","hiển thị tất cả các job đang chạy hoặc chờ chạy","hiển thị thông tin chi tiết về job","hủy job (ví dụ: scancel 2534640)","scancel","scontrol show jobid","slurmtop","sprio","squeue","squeue --start","squeue -j","squeue -u","Sự miêu tả","xem job của bạn đang chạy hoặc đang chờ chạy","Yêu cầu"]},{"l":"Sau khi job hoàn thành","p":["Lệnh","Mô tả","seff","xem kết quả job đã hoàn thành","shistory","hiển thị lịch sử job của bạn","jobstats","xem số liệu chính xác về bộ nhớ, CPU và GPU từ các job. Xem thêm về số liệu thống kê job."]},{"l":"Your First Slurm Job","p":["Nếu bạn chưa quen với cụm HPC hoặc Slurm thì hãy xem hướng dẫn này để chạy job đầu tiên của bạn."]},{"l":"Terminology","p":["Khi bạn SSH đến một cụm, bạn đang kết nối với head node, node này được tất cả người dùng chia sẻ. Việc chạy job trên nút đăng nhập bị CẤM. Các job hàng loạt và tương tác phải được gửi từ head node đến bộ lập lịch job của Slurm bằng cách sử dụng lệnh “srun”, \"sbatch\" và \"salloc\". Sau khi xếp hàng đợi, các job sẽ được gửi đến các compute node thực hiện job tính toán thực tế.","Mỗi CPU có nhiều lõi CPU. Chạy lệnh \"snodes\" và xem cột \"CPUS\" ở đầu ra để xem số lượng lõi CPU trên mỗi nút cho một cụm nhất định. Bạn sẽ thấy các giá trị như 28, 32, 40, 96 và 128. Nếu job của bạn yêu cầu số lượng lõi CPU trên mỗi nút trở xuống thì hầu như bạn luôn nên sử dụng --nodes=1 trong tập lệnh Slurm của mình. Các giá trị cho --ntasks và --cpus-per-task được giải thích rõ nhất bằng các ví dụ bên dưới.","Lệnh --time đặt thời gian chạy tối đa cần thiết cho job của bạn. Bạn nên đặt giá trị này một cách chính xác nhưng hãy tính thêm 20% vì job sẽ bị hủy nếu nó không hoàn thành trước khi đạt đến giới hạn. Để đặt yêu cầu bộ nhớ của job bằng cách sử dụng --mem-per-cpu hoặc --mem."]},{"l":"Interactive Allocations with salloc","p":["Nút đăng nhập của cụm chỉ có thể được sử dụng cho job tương tác rất nhẹ sử dụng tối đa 10% máy (lõi CPU và bộ nhớ) trong tối đa 10 phút. Điều này được thực thi nghiêm ngặt vì việc vi phạm quy tắc này thường có thể ảnh hưởng xấu đến job của những người dùng khác. Job tương tác chuyên sâu phải được thực hiện trên các nút điện toán bằng lệnh salloc. Để hoạt động tương tác trên nút điện toán có 1 lõi CPU và 4 GB bộ nhớ trong 20 phút, hãy sử dụng lệnh sau:","Giống như các job hàng loạt, việc phân bổ tương tác sẽ đi qua hệ thống xếp hàng. Điều này có nghĩa là khi clusterr bận, bạn sẽ phải đợi trước khi được cấp phân bổ. Bạn sẽ thấy đầu ra như sau:","Sau thời gian chờ đợi, bạn sẽ được đưa vào một shell trên compute node nơi bạn có thể bắt đầu làm việc một cách tương tác. Lưu ý rằng việc phân bổ của bạn sẽ chấm dứt khi đạt đến giới hạn thời gian. Bạn có thể sử dụng lệnh thoát để kết thúc phiên và quay lại nút đăng nhập bất cứ lúc nào.","GPU","Để yêu cầu một nút có GPU:","Graphics","Nếu bạn đang làm việc với đồ họa thì hãy bật chuyển tiếp X11 (và xem các yêu cầu bổ sung):"]},{"l":"GPU Job","p":["Xem số lượng GPU trên mỗi cluster tại trang thông tin cluster đó. Để sử dụng GPU trong job, hãy thêm câu lệnh SBATCH với tùy chọn --gres:","Ví dụ: để sử dụng bốn GPU trên mỗi nút, dòng thích hợp sẽ là:","QUAN TRỌNG: Chỉ những mã được viết rõ ràng để chạy trên GPU mới có thể tận dụng GPU. Việc thêm tùy chọn --gres vào tập lệnh Slurm cho mã chỉ dành cho CPU sẽ không tăng tốc thời gian thực thi nhưng sẽ lãng phí tài nguyên, tăng thời gian xếp hàng và giảm mức độ ưu tiên của lần gửi job tiếp theo. Hơn nữa, một số mã chỉ được viết để sử dụng một GPU duy nhất nên tránh yêu cầu nhiều GPU trừ khi mã của bạn có thể sử dụng chúng. Nếu mã có thể sử dụng nhiều GPU thì bạn nên tiến hành phân tích tỷ lệ để tìm ra số lượng GPU tối ưu để sử dụng.","Xem thông tin hệ thống để biết về cấu hình của server. Lưu ý rằng một số mã yêu cầu sử dụng nhiều lõi CPU cùng với GPU để có hiệu suất tối ưu."]},{"i":"jobarrays","l":"Job Arrays","p":["Mảng job được dùng để chạy cùng một job nhiều lần mà chỉ có những khác biệt nhỏ giữa các job. Ví dụ: giả sử bạn cần chạy 100 job, mỗi job có một giá trị hạt giống khác nhau cho trình tạo số ngẫu nhiên. Hoặc có thể bạn muốn chạy cùng một tập lệnh phân tích dữ liệu cho từng file trong tổng số 50 files. Job array là lựa chọn tốt nhất cho những trường hợp như vậy.","Dưới đây là một ví dụ về tập lệnh Slurm để chạy Python trong đó có 5 job:","Dòng chính trong tập lệnh Slurm ở trên là:","Trong ví dụ này, tập lệnh Slurm sẽ chạy năm job. Mỗi job sẽ có giá trị SLURM_ARRAY_TASK_ID khác nhau (tức là 0, 1, 2, 3, 4). Giá trị của SLURM_ARRAY_TASK_ID có thể được sử dụng để phân biệt các job trong mảng. Xem ví dụ đầy đủ về Python. Người ta có thể chuyển SLURM_ARRAY_TASK_ID cho tệp thực thi dưới dạng tham số dòng lệnh hoặc tham chiếu nó dưới dạng biến môi trường. Sử dụng cách tiếp cận thứ hai, một vài dòng đầu tiên của tập lệnh Python (được gọi là myscript.py ở trên) có thể trông như thế này:"]},{"l":"Các job sử dụng bộ nhớ lớn","p":["Một lợi thế của việc sử dụng cụm HPC trên máy tính xách tay hoặc máy trạm của bạn là lượng RAM lớn có sẵn trên mỗi nút. Ví dụ: trên một số cụm, bạn có thể chạy một job với bộ nhớ 100 GB. Điều này có thể rất hữu ích khi làm việc với một tập dữ liệu lớn. Để tìm hiểu xem mỗi nút có bao nhiêu bộ nhớ, hãy chạy lệnh snodes và xem cột MEMORY tính bằng megabyte.","Ví dụ trên chạy tập lệnh Python sử dụng 1 lõi CPU và 100 GB bộ nhớ. Trong tất cả các tập lệnh Slurm, bạn nên sử dụng giá trị chính xác cho bộ nhớ cần thiết nhưng thêm 20% để đảm bảo an toàn. Để biết thêm, hãy xem Phân bổ bộ nhớ."]},{"l":"Báo cáo (hiện tại chưa hỗ trợ)","p":["Nếu bạn đưa chỉ thị thư SBATCH bên dưới vào tập lệnh Slurm thì bạn sẽ nhận được email sau khi mỗi job kết thúc:","Dưới đây là một báo cáo mẫu:","Báo cáo cung cấp thông tin về thời gian chạy, mức sử dụng CPU, mức sử dụng bộ nhớ, v.v. Bạn nên kiểm tra các giá trị này để xác định xem bạn có đang sử dụng tài nguyên đúng cách hay không. Xem trang Thống kê job để biết thêm thông tin.","Thời gian xếp hàng của bạn một phần được xác định bởi lượng tài nguyên bạn yêu cầu. Giá trị chia sẻ công bằng của bạn, một phần quyết định mức độ ưu tiên của job tiếp theo của bạn, sẽ giảm tỷ lệ với lượng tài nguyên bạn đã sử dụng trong 30 ngày trước đó."]},{"l":"Tài liệu đọc thêm","p":["external resources for learning Slurm."]}],[{"l":"Kiểm tra trạng thái Job*","p":["Sau khi nộp job lên SLURM bằng lệnh sbatch, nên kiểm tra danh sách các job đang chạy hoặc đang chờ trong hàng đợi, người dùng có thể sử dụng lệnh:","Để xem thời gian bắt đầu dự kiến cho job đang chờ xử lý:","Trường hợp ngay sau khi nộp job bằng sbatch mà không thấy job của mình nằm trong hàng đợi, khả năng cao là job đã bị lỗi và cần kiểm tra file output."]},{"l":"Các lệnh SLURM thường dùng*","p":["Dưới đây là một số lệnh SLURM phổ biến cần sử dụng:","squeue: Kiểm tra trạng thái của các job.","scancel: Hủy một job đang chạy hoặc trong hàng đợi.","scontrol: Hiển thị thông tin chi tiết về một job cụ thể.","sinfo: Hiển thị thông tin các nodes."]}],[{"l":"Lưu ý quan trọng khi sử dụng hệ thống","p":["Toàn bộ source code và dữ liệu cần được lưu ở đường dẫn /media/{username} hoặc /media02/{username}","Tuyệt đối không lưu dữ liệu ở /home/{username} do không có nhiều dung lượng lưu trữ tại đây.","Lưu ý: đối với thư mục ẩn (ví dụ như /home/{username}/.cache) cũng phải được chuyển qua /media/{username}.","Không chạy bất kỳ chương trình xử lý data / huấn luyện mô hình nào trên các login nodes, vì login nodes không có cấu hình cao, và được nhiều nhóm nghiên cứu sử dụng chung.","Sau khi hoàn tất sử dụng, ngắt kết nối (ssh), đặc biệt đối với các nhóm sử dụng VSCode, vì nếu không ngắt kết nối sẽ làm chậm kết nối chung của tất cả các nhóm sử dụng.","Đảm bảo rằng các yêu cầu về tài nguyên được chỉ định chính xác và vừa đủ trước khi gửi job.","Tính toán thêm 20% cho thời gian dự kiến của job để tránh việc job bị hủy do vượt quá giới hạn thời gian.","Kiểm tra các thông báo từ SLURM để tối ưu hóa yêu cầu tài nguyên cho các job trong tương lai.","Thường xuyên backup mã nguồn và dữ liệu (2 ngày / lần), hệ thống có thể sẽ xóa dữ liệu định kỳ (có thông báo trước 3-5 ngày làm việc)."]}]]